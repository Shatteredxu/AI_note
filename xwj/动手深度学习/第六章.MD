# 循环神经网络
循环神经⽹络是为更好地**处理时序信息而设计的**。它引⼊**状态变量**来存储过去的信息，并⽤其与当前的输⼊共同决定当前的输出。常⽤于**处理序列数据**，如⼀段⽂字或声⾳、购物或观影的顺序，甚⾄是图像中的⼀⾏或⼀列像素，多用于**语⾔模型、⽂本分类、机器翻
译、语⾳识别、图像分析、⼿写识别和推荐系统**

## 语言模型
1. 语言模型的计算

计算一段文本序列的概率
![image](http://qcihljxys.bkt.clouddn.com/1594623554%281%29.png)通过条件概率一步一步计算，为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即语⾔模型参数
2. n元语法

当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈**指数级增加**。n元语法通过**⻢尔可夫假设**简化了语⾔模型的计算。⻢尔可夫假设基于⼀个词的出现只与前⾯n个词相关，即n阶⻢尔可夫链（Markov chain of order n）。如果n = 1，那么有P(w 3 | w 1 ,w 2 ) = P(w 3 | w 2 )**也就是一个词的出现只与前面一个词相关。**
## 循环神经网络
上⼀节介绍的n元语法中，时间步t的词w基于前⾯所有词的条件概率只考虑了**最近时间步的n −1个词**。如果要考虑⽐t −(n−1)更早时间步的词对w的可能影响，我们需要增⼤n。但这样模型
参数的数量将随之呈**指数级增⻓**（可参考上⼀节的练习）。  
本节将介绍**循环神经⽹络**。它并⾮刚性地记忆所有固定⻓度的序列，而是**通过隐藏状态来存储之前时间步的信息**。⾸先我们回忆⼀下前⾯介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经⽹络。

1. 含隐藏状态的循环神经⽹络

时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：

```math
H_t = ϕ(X_t W_xh + H_t−1 W_hh + b_h ).
```
与多层感知机相⽐，我们在这⾥添加了

```math
H_{t-1},w_{hh}
```
能够捕捉截⾄当前时间步的序列的历史信息，就像是神经⽹络当前时间步的状态或记忆⼀样
2. 应⽤：基于字符级循环神经⽹络的语⾔模型

在训练时，我们对每个时间步的输出层输出使⽤**softmax运算**，然后使⽤**交叉熵损失函数**来计算它与标签的误差。在图6.2中，由于隐藏层中隐藏状态的循环计算，时间步3的输
出O 3 取决于⽂本序列“想”“要”“有”。由于训练数据中该序列的下⼀个词为“直”，时间步3的
损失将取决于该时间步基于序列“想”“要”“有”⽣成下⼀个词的概率分布与该时间步的标签
“直”
![image](http://qcihljxys.bkt.clouddn.com/1594625537%281%29.png)
##  语⾔模型数据集（周杰伦专辑歌词）
本节将介绍如何**预处理**⼀个语⾔模型数据集，并将其**转换成字符级循环神经⽹络所需要的输⼊格式**。为此，我们收集了周杰伦从第⼀张专辑《Jay》到第⼗张专辑《跨时代》中的歌词，并在后⾯⼏节⾥应⽤循环神经⽹络来训练⼀个语⾔模型。当模型训练好后，我们就可以⽤这个模型来创作歌词。
1. 获取字符集合
```
# 1.获取字符集合
with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip') as zin:
    with zin.open('jaychou_lyrics.txt') as f:
        corpus_chars = f.read().decode('utf-8')
print(corpus_chars[:40])
corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ')
corpus_chars = corpus_chars[0:10000]
```
2. 建立字符索引

```
# 2.建立字符索引
idx_to_char = list(set(corpus_chars))
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])
vocab_size = len(char_to_idx)
# 获取文字和索引对应关系
corpus_indices = [char_to_idx[char] for char in corpus_chars]
sample = corpus_indices[:20]
print('chars:', ''.join([idx_to_char[idx] for idx in sample]))
print('indices:', sample)
```
3. 时序数据的采样

- 随机采样
下⾯的代码每次从数据⾥随机采样⼀个小批量。其中批量⼤小batch_size指每个小批量的样本
数，num_steps为每个样本所包含的时间步数。在随机采样中，每个样本是原始序列上任意截取
的⼀段序列。相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个
小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。在训练模型时，每次随机采样
前都需要重新初始化隐藏状态

```
1. yield的用法？
2. shuffle使用
```

```
def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):
    # 减1是因为输出的索引是相应输⼊的索引加1
    num_examples = (len(corpus_indices) - 1) // num_steps
    epoch_size = num_examples // batch_size
    example_indices = list(range(num_examples))
    random.shuffle(example_indices)
    # 返回从pos开始的⻓为num_steps的序列
    def _data(pos):
        return corpus_indices[pos: pos + num_steps]
    for i in range(epoch_size):
        # 每次读取batch_size个随机样本
        i = i * batch_size
        batch_indices = example_indices[i: i + batch_size]
        X = [_data(j * num_steps) for j in batch_indices]
        Y = [_data(j * num_steps + 1) for j in batch_indices]
        yield nd.array(X, ctx), nd.array(Y, ctx)
```
- 相邻采样

采用reshpe
```
 # 相邻采样
def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):
    corpus_indices = nd.array(corpus_indices, ctx=ctx)
    data_len = len(corpus_indices)
    batch_len = data_len // batch_size
    indices = corpus_indices[0: batch_size*batch_len].reshape((
            batch_size, batch_len))
    epoch_size = (batch_len - 1) // num_steps
    for i in range(epoch_size):
        i = i * num_steps
        X = indices[:, i: i + num_steps]
        Y = indices[:, i + 1: i + num_steps + 1]
        yield X, Y
```
## 循环神经⽹络的从零开始实现
1. one-hot向量

为了将词表⽰成向量输⼊到神经⽹络，⼀个简单的办法是使⽤**one-hot向量**。假设词典中不同字符的数量为N（即词典⼤小vocab_size），每个字符已经同⼀个从0到N −1的连续整数值索引⼀⼀对应。如果⼀个字符的索引是整数i,那么我们创建⼀个**全0的⻓为N的向量**，++并将其位置为i的元素设成1。该向量就是对原字符的one-hot向量++。

```
def to_onehot(X, size): # 本函数已保存在d2lzh包中⽅便以后使⽤
    return [nd.one_hot(x, size) for x in X.T]
```
2. 初始化模型参数

```
def get_params():
    def _one(shape):
        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)
    # 隐藏层参数
    W_xh = _one((num_inputs, num_hiddens))
    W_hh = _one((num_hiddens, num_hiddens))
    b_h = nd.zeros(num_hiddens, ctx=ctx)
    # 输出层参数
    W_hq = _one((num_hiddens, num_outputs))
    b_q = nd.zeros(num_outputs, ctx=ctx)
    # 附上梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
    param.attach_grad()
    return params
```
3. 定义模型

⾸先定义init_rnn_state函数来返回初始化的隐藏状态
```
def init_rnn_state(batch_size, num_hiddens, ctx):
    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )
```
下⾯的rnn函数定义了在⼀个时间步⾥如何计算隐藏状态和输出。这⾥的激活函数使⽤了tanh函
数。当元素在实数域上均匀分布时，tanh函数值的均值为0。

```
def rnn(inputs, state, params):
    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)
        Y = nd.dot(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H,)
```
4. 定义预测函数
5. 裁剪梯度

循环神经⽹络中较容易出现梯度衰减或梯度爆炸。我们会在“通过时间反向传播”⼀节中解释原
因。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元
素拼接成⼀个向量 g，并设裁剪的阈值是θ。裁剪后的梯度

```math
min(\frac{\theta }{||g||},1)g
```
的L 2 范数不超过θ

```
def grad_clipping(params, theta, ctx):
    norm = nd.array([0], ctx)
    for param in params:
        norm += (param.grad ** 2).sum()
    norm = norm.sqrt().asscalar()
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
```
6. 困惑度

我们通常使⽤**困惑度（perplexity）**来评价语⾔模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，
- 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；
- 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷；
- 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。

7. 定义模型训练函数
跟之前章节的模型训练函数相⽐，这⾥的模型训练函数有以下⼏点不同：
    1. 使⽤困惑度评价模型。
    2. 在迭代模型参数前裁剪梯度。
    3. 对时序数据采⽤不同采样⽅法将导致隐藏状态初始化的不同。相关讨论可参考“语⾔模型数据集（周杰伦专辑歌词）”⼀节

```
def train_and_predict_rnn(rnn, get_params, init_rnn_state,                         num_hiddens,
                        vocab_size, ctx, corpus_indices, idx_to_char,
                        char_to_idx, is_random_iter, num_epochs, num_steps,
                        lr, clipping_theta, batch_size, pred_period,
                        pred_len, prefixes):
    if is_random_iter:
        data_iter_fn = d2l.data_iter_random
    else:
        data_iter_fn = d2l.data_iter_consecutive
    params = get_params()
    loss = gloss.SoftmaxCrossEntropyLoss()
    for epoch in range(num_epochs):
        if not is_random_iter: # 如使⽤相邻采样，在epoch开始时初始化隐藏状态
            state = init_rnn_state(batch_size, num_hiddens, ctx)
        l_sum, n, start = 0.0, 0, time.time()
        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)
        for X, Y in data_iter:
            if is_random_iter: # 如使⽤随机采样，在每个⼩批量更新前初始化隐藏状态
            state = init_rnn_state(batch_size, num_hiddens, ctx)
            else: # 否则需要使⽤detach函数从计算图分离隐藏状态
                for s in state:
                    s.detach()
            with autograd.record():
                inputs = to_onehot(X, vocab_size)
                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵
                (outputs, state) = rnn(inputs, state, params)
                # 拼接之后形状为(num_steps * batch_size, vocab_size)
                outputs = nd.concat(*outputs, dim=0)
                # Y的形状是(batch_size, num_steps)，转置后再变成⻓度为
                # batch * num_steps 的向量，这样跟输出的⾏⼀⼀对应
                y = Y.T.reshape((-1,))
                # 使⽤交叉熵损失计算平均分类误差
                l = loss(outputs, y).mean()
            l.backward()
            grad_clipping(params, clipping_theta, ctx) # 裁剪梯度
            d2l.sgd(params, lr, 1) # 因为误差已经取过均值，梯度不⽤再做平均
            l_sum += l.asscalar() * y.size
            n += y.size
        if (epoch + 1) % pred_period == 0:
            print('epoch %d, perplexity %f, time %.2f sec' % (
                epoch + 1, math.exp(l_sum / n), time.time() - start))
            for prefix in prefixes:
                print(' -', predict_rnn(
                    prefix, pred_len, rnn, params, init_rnn_state,
                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))
```
## 循环神经⽹络的简洁实现
1. 定义模型

```python
class RNNModel(nn.Block):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.dense = nn.Dense(vocab_size)
    def forward(self, inputs, state):
        # 将输⼊转置成(num_steps, batch_size)后获取one-hot向量表⽰
        X = nd.one_hot(inputs.T, self.vocab_size)
        Y, state = self.rnn(X, state)
        # 全连接层会⾸先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出
        # 形状为(num_steps * batch_size, vocab_size)
        output = self.dense(Y.reshape((-1, Y.shape[-1])))
        return output, state
    def begin_state(self, *args, **kwargs):
        return self.rnn.begin_state(*args, **kwargs)
```
2. 训练模型

```
def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,
char_to_idx):
    # 使⽤model的成员函数来初始化隐藏状态
    state = model.begin_state(batch_size=1, ctx=ctx)
    output = [char_to_idx[prefix[0]]]
    for t in range(num_chars + len(prefix) - 1):
        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))
        (Y, state) = model(X, state) # 前向计算不需要传⼊模型参数
    if t < len(prefix) - 1:
        output.append(char_to_idx[prefix[t + 1]])
    else:
        output.append(int(Y.argmax(axis=1).asscalar()))
    return ''.join([idx_to_char[i] for i in output])
```
##  通过时间反向传播
本节将介绍循环神经⽹络中梯度的计算和存储⽅法，即**通过时间反向传播**（back-propagation through time）

通过时间反向传播其实是**反向传播在循环神经⽹络中的具体应⽤**。++我们需要将循环神经⽹络**按时间步展开**，从而
得到模型变量和参数之间的依赖关系，并依据**链式法则应⽤反向传播计算并存储梯度**。++


---
==这个地方以后找个书籍，详细了解一下==

##  ⻔控循环单元（GRU）
我们发现，当时间步数较⼤或者时间步较小时，
循环神经⽹络的梯度较容易出现**衰减或爆炸**。虽然裁剪梯度可以应对梯度爆炸，但**⽆法解决梯度衰减**的问题。通常由于这个原因，循环神经⽹络在实际中较难捕捉时间序列中时间步距离较⼤的
依赖关系。
⻔控循环神经⽹络（gatedrecurrentneuralnetwork）的提出，正是为了更好地**捕捉时间序列中时间步距离较⼤的依赖关系**。它通过可以学习的⻔来控制信息的流动。其中，⻔控循环单元（gatedrecurrent unit，GRU）是⼀种常⽤的⻔控循环神经⽹络 [1, 2]

```
* 它引⼊了重置⻔（reset gate）和更新⻔（update gate）的概念
* 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
* 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
```
1. 初始化模型参数

```
def get_params():
    def _one(shape):
        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)
    def _three():
        return (_one((num_inputs, num_hiddens)),
    _one((num_hiddens, num_hiddens)),
    nd.zeros(num_hiddens, ctx=ctx))
    W_xz, W_hz, b_z = _three() # 更新⻔参数
    W_xr, W_hr, b_r = _three() # 重置⻔参数
    W_xh, W_hh, b_h = _three() # 候选隐藏状态参数
    # 输出层参数
    W_hq = _one((num_hiddens, num_outputs))
    b_q = nd.zeros(num_outputs, ctx=ctx)
    # 附上梯度
    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.attach_grad()
    return params
```
2. 定义模型

```
# 它返回由⼀个形状为(批量⼤小, 隐藏单元个数)的值
为0的NDArray组成的元组
def init_gru_state(batch_size, num_hiddens, ctx):
    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )
```

```
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = nd.sigmoid(nd.dot(X, W_xz) + nd.dot(H, W_hz) + b_z)
        R = nd.sigmoid(nd.dot(X, W_xr) + nd.dot(H, W_hr) + b_r)
        H_tilda = nd.tanh(nd.dot(X, W_xh) + nd.dot(R * H, W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = nd.dot(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H,)
```
3.  简洁实现

在Gluon中我们直接调⽤rnn模块中的GRU类即可。

```
gru_layer = rnn.GRU(num_hiddens)
model = d2l.RNNModel(gru_layer, vocab_size)
d2l.train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,
            corpus_indices, idx_to_char, char_to_idx,
            num_epochs, num_steps, lr, clipping_theta,
            batch_size, pred_period, pred_len, prefixes)
```

```
* ⻔控循环神经⽹络可以更好地捕捉时间序列中时间步距离较⼤的依赖关系。
* ⻔控循环单元引⼊了⻔的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。它包括重置⻔、更新⻔、候选隐藏状态和隐藏状态。
* 重置⻔有助于捕捉时间序列⾥短期的依赖关系。
* 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
```
## ⻓短期记忆（LSTM）
LSTM 中引⼊了3个⻔，即**输⼊⻔**（input gate）、遗忘⻔（forget gate）和**输出⻔**（output gate），
以及与**隐藏状态形状相同的记忆细胞**（某些⽂献把记忆细胞当成⼀种特殊的隐藏状态），从而记录额外的信息

==还需要着重了解==
1. 初始化模型参数
添加了输入门，遗忘门，输出门，候选记忆细胞参数
2. 定义模型

首先：在初始化函数中，⻓短期记忆的隐藏状态需要返回额外的形状为(批量⼤小, 隐藏单元个数)的值为0的记忆细胞。
然后定义lstm模型

```
注意sigmod函数和tanh函数
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
    W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)
        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)
        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)
        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * C.tanh()
        Y = nd.dot(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H, C)
```
3. 简洁实现

```
# 在Gluon中我们可以直接调⽤rnn模块中的LSTM类
lstm_layer = rnn.LSTM(num_hiddens)
model = d2l.RNNModel(lstm_layer, vocab_size)
d2l.train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,
            corpus_indices, idx_to_char, char_to_idx,
            num_epochs, num_steps, lr, clipping_theta,
            batch_size, pred_period, pred_len, prefixes)
```

```
* ⻓短期记忆的隐藏层输出包括隐藏状态和记忆细胞。只有隐藏状态会传递到输出层。
* ⻓短期记忆的输⼊⻔、遗忘⻔和输出⻔可以控制信息的流动。
* ⻓短期记忆可以应对循环神经⽹络中的**梯度衰减问题**，并更好地捕捉时间序列中时间步距离较⼤的依赖关系。
```
## 深度循环神经⽹络
在深度学习应⽤⾥，我们通常会⽤到**含有多个隐藏层的循环神经⽹络**，也称作深度循环神经⽹络。在深度循环神经⽹络中，隐藏状态的信息不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层。
## 双向循环神经⽹络

```
* 之前介绍的循环神经⽹络模型都是假设**当前时间步是由前⾯的较早时间步的序列决定的**，因此它们都将信息通过隐藏状态从前往后传递。
* 有时候，当前时间步也可能由后⾯时间步决定。
例如，当我们写下⼀个句⼦时，可能会**根据句⼦后⾯的词来修改句⼦前⾯的⽤词**。
双向循环神经⽹络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息  

* 双向循环神经⽹络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）。
```
