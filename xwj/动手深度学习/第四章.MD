# 深度学习计算
## 模型构造
本章主要介绍了模型构造的另一种方法，基于Block类的模型构造⽅法：它能让模型构造更加灵活。
1. 继承Block类来构造模型
**Block类**是nn模块⾥提供的⼀个模型构造类，我们可以继承它来定义我们想要的模型。下⾯继承Block类构造本节开头提到的多层感知机

```
from mxnet import nd
from mxnet.gluon import nn
class MLP(nn.Block):
    # 声明带有模型参数的层，这⾥声明了两个全连接层
    def __init__(self, **kwargs):
        # 调⽤MLP⽗类Block的构造函数来进⾏必要的初始化。这样在构造实例时还可以指定其他函数
        # 参数，如“模型参数的访问、初始化和共享”⼀节将介绍的模型参数params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu') # 隐藏层
        self.output = nn.Dense(10) # 输出层
        # 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出
    def forward(self, x):
        return self.output(self.hidden(x))
```

继承block后系统将通过⾃动求梯度而⾃动⽣成反向传播所需的backward函数
2. Sequential类继承⾃Block类
它提供**add函数**来逐⼀添加串联的Block⼦类实例，而模型的前向计算就是将这些实例按添加的顺序逐⼀计算
```
class MySequential(nn.Block):
    def __init__(self, **kwargs):
        super(MySequential, self).__init__(**kwargs)
    def add(self, block):
        # block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的
        # 成员变量_children⾥，其类型是OrderedDict。当MySequential实例调⽤
        # initialize函数时，系统会⾃动对_children⾥所有成员初始化
        self._children[block.name] = block
    def forward(self, x):
        # OrderedDict保证会按照成员添加时的顺序遍历成员
        for block in self._children.values():
            x = block(x)
        return x
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
print(net(X))
```
3. 构造复杂的模型
虽然Sequential类可以使模型构造更加简单，且不需要定义forward函数，但直接继承Block类可以极⼤地拓展模型构造的灵活性

```
class FancyMLP(nn.Block):
    def __init__(self, **kwargs):
        super(FancyMLP, self).__init__(**kwargs)
        # 使⽤get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）
        self.rand_weight = self.params.get_constant(
            'rand_weight', nd.random.uniform(shape=(20, 20)))
        self.dense = nn.Dense(20, activation='relu')
    def forward(self, x):
        x = self.dense(x)
        # 使⽤创建的常数参数，以及NDArray的relu函数和dot函数
        x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)
        # 复⽤全连接层。等价于两个全连接层共享参数
        x = self.dense(x)
        # 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较
        while x.norm().asscalar() > 1:
            x /= 2
        if x.norm().asscalar() < 0.8:
            x *= 10
        return x.sum()

```
       
添加了多各层，以及relu激活函
##  模型参数的访问、初始化和共享
1.模型参数访问
```
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize() # 使⽤默认初始化⽅式
X = nd.random.uniform(shape=(2, 20))
Y = net(X) # 前向计算
# 对于使⽤Sequential类构造的神经⽹络，我们可以通过⽅括号[]来访问⽹络的任⼀层
print( net[0].params, type(net[0].params))#查看网络所有参数
print( net[0].params['dense0_weight'], net[0].weight)# 通过名字来访问字典⾥的元素，也可以直接使⽤它的变量名
print(net[0].weight.data(),net[0].weight.grad())# 查看参数和梯度
print(net[1].bias.data())#访问其他层的参数，如输出层的偏差值
print( net.collect_params())#使⽤collect_params函数来获取net变量所有嵌套（例如通过add函数嵌套）的层所包含的所有参数
print(net.collect_params('.*weight'))# 可以通过正则表达式来匹配参数名，从而筛选需要的参数
```
2.初始化模型参数
```
# 2.初始化模型参数
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)# # ⾮⾸次对模型初始化需要指定force_reinit为真
print(net[0].weight.data()[0])
net.initialize(init=init.Constant(1), force_reinit=True)
print(net[0].weight.data()[0])#初始化权重参数
#对某个特定的参数进行初始化
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)#Xavier随机初始化
print(net[0].weight.data()[0])
```
3. ⾃定义初始化⽅法
实现一个Initializer类，能够像使⽤其他初始化⽅法那样使⽤它，我们只需要实现_init_weight这个函数，并将其传⼊的NDArray修改成初始化的结果

```
# 3.自定义参数
class MyInit(init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
        data *= data.abs() >= 5
net.initialize(MyInit(), force_reinit=True)
net[0].weight.data()[0]
```
还可以通过Parameter类的set_data函数来直接改写模型参数

```
net[0].weight.set_data(net[0].weight.data() + 1)
net[0].weight.data()[0]
```
4.共享模型参数  

构造层的时候指定使⽤特定的参数。如果不同层使⽤同⼀份参数，那么它们在前向计算和反向传播时都会共享相同的参数。

```
# 4.共享模型参数
net = nn.Sequential()
shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'),
    shared,
    nn.Dense(8, activation='relu', params=shared.params),#使用shared层的参数
    nn.Dense(10))
net.initialize()
X = nd.random.uniform(shape=(2, 20))
net(X)
print(net[1].weight.data()[0] == net[2].weight.data()[0])
```
构造第三层时使用了第二层的参数，所以在反向传播计算时，第⼆隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()⾥
##  模型参数的延后初始化
- 为什么模型要进行延后初始化呢？  
答：主要是为了让模型的构造更加简单。例如，我们无须人工推测每个模型的输入个数（尤其层数多的时候，不推测直接写也很麻烦），**模型的初始化是在第一次前向计算时初始化的**
系统能够根据输⼊的形
状⾃动推断出所有层的权重参数的形状。系统在创建这些参数之后，调⽤MyInit实例对它们进⾏初始化，然后才进⾏前向计算

- 延后初始化也可能会带来⼀定的困惑。在第⼀次前向计算之前，我们⽆法直接操作模型参数，例如⽆法使⽤data函数和set_data函数
来获取和修改参数。因此，我们经常会额外做⼀次前向计算来迫使参数被真正地初始化
###  避免延后初始化
如果系统在调⽤initialize函数时能够知道所有参数的形状，那么延后初始化就不会发⽣

```
** 第⼀种情况是我们要对已初始化的模型重新初始化时。因为参数形状不会发⽣变化，所以系统能够⽴即进⾏重新初始化
** 第⼆种情况是我们在创建层的时候指定了它的输⼊个数，使系统不需要额外的信息来推测参数形
状
net = nn.Sequential()
net.add(nn.Dense(256, in_units=20, activation='relu'))
net.add(nn.Dense(10, in_units=256))
net.initialize(init=MyInit())
这里通过in_units来指定每个全连接层的输⼊个数，使初始化能够在initialize函数被调⽤时⽴即发⽣
```
## ⾃定义层（含权重参数和偏差参数的全连接层？）
本节将介绍如何使⽤NDArray来⾃定义⼀个Gluon的层，从而可以被重复调⽤
1. 通过继承Block类⾃定义了⼀个将输⼊减掉均值后输出的层，并将层的计算定义在了forward函数⾥
2. 利⽤Block类⾃带的ParameterDict类型的成员变量params。
它是⼀个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。我们可以通过get函数从ParameterDict创建Parameter实例

```
 class MyDense(nn.Block):
    # units为该层的输出个数，in_units为该层的输⼊个数
    def __init__(self, units, in_units, **kwargs):
        super(MyDense, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=(in_units, units))
        self.bias = self.params.get('bias', shape=(units,))
    def forward(self, x):
        linear = nd.dot(x, self.weight.data()) + self.bias.data()
        return nd.relu(linear)
```
含权重参数和偏差参数的全连接层。它使⽤ReLU函数作为激活函数。其中in_units和units分别代表输⼊个数和输出个数
##  读取和存储
实际中，
我们有时需要把训练好的模型部署到**很多不同的设备**。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使⽤
1. 读写NDArray
主要是**save函数**和**load函数**
```python
# 基础使用
x = nd.ones(3)
nd.save('x', x)
 x2 = nd.load('x')
```
2. 读写Gluon模型的参数
除NDArray以 外， 我 们 还 可 以 **读 写Gluon模 型 的 参 数**。Gluon的Block类 提 供了**save_parameters函 数** 和**load_parameters函 数** 来 读 写 模 型 参 数
通过net.save_parameters(filename)来保存，使用net2.load_parameters(filename)来恢复
##  GPU计算
我们可以在创建NDArray的时候通过ctx参数指定存储设备。下⾯我们将NDArray变量a创建在gpu(0)上。注意，在打印a时，设备信息变成了@gpu(0)。创建在显存上的NDArray只消耗同⼀块显卡的显存。我们可以通过nvidia-smi命令查看显存的使⽤情况

```
a = nd.array([1, 2, 3], ctx=mx.gpu())
```
有两块gpu时可以指定 ctx=mx.gpu(1)来选择其他gpu

除了在创建时指定，我们也可以通过copyto函数和as_in_context函数在设备之间传输数据。
下⾯我们将内存上的NDArray变量x复制到gpu(0)上

```
y = x.copyto(mx.gpu())
z = x.as_in_context(mx.gpu())
```
需要区分的是，如果源变量和⽬标变量的context⼀致，as_in_context函数使⽬标变量和源变量共享源变量的内存或显存
3. GPU上的计算  
MXNet要求计算的所有输⼊数据都在内存或同⼀块显卡的显存上。这样设计的原因是CPU和不同的GPU之间的数据交互通常⽐较耗时，如果不在一起会出现错误信息
4.  Gluon的GPU计算
```
net.initialize(ctx=mx.gpu())#初始化
```
