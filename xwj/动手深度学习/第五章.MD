# 卷积神经网络
包括从最早提出的**AlexNet**，使用重复元素的⽹络**（VGG**）、⽹络中的⽹络**（NiN）**、含并⾏
连结的⽹络**（GoogLeNet）**、残差⽹络**（ResNet）**和稠密连接⽹络**（DenseNet）**
## 二维卷积层
⼆维卷积层:它有⾼和宽两个空间维度，常⽤来处理图像数据
1. ⼆维互相关运算  

⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相关运算输出⼀个⼆维数组,这个二维核通常称为**卷积核或者过滤器**，在⼆维互相关运算中，卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组⾼和宽分别为2，其中的4个元素由⼆维互相关运算得出：![image](http://qcihljxys.bkt.clouddn.com/1594022628%281%29.png)

```math
0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19,  

1 × 0 + 2 × 1 + 4 × 2 + 5 × 3 = 25,

3 × 0 + 4 × 1 + 6 × 2 + 7 × 3 = 37,

4 × 0 + 5 × 1 + 7 × 2 + 8 × 3 = 43
```

```python
def corr2d(X,K):
    h,w = K.shape
    Y = nd.zeros((X.shape[0]-h+1,X.shape[1]-w+1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] =(X[i: i + h, j: j + w] * K).sum()#这句话怎样理解
    return Y

X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
K = nd.array([[0, 1], [2, 3]])
print(corr2d(X, K))
```
2. 二维卷积层

```
# 定义一个二维卷积层
class Conv2D(nn.Block):
    def __init__(self, kernel_size, **kwargs):
        super(Conv2D, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=kernel_size)
        self.bias = self.params.get('bias', shape=(1,))
    def forward(self, x):
        return corr2d(x, self.weight.data()) + self.bias.data()
X= nd.ones((6,8))
X[:,2:6] = 0
K = nd.array([[1, -1]])
Y = corr2d(X, K)
print(Y)
```

3. 通过数据学习核数组

使用了mxnet中的库来建立卷积层

```
 # 构造⼀个输出通道数为1（将在“多输⼊通道和多输出通道”⼀节介绍通道），核数组形状是(1, 2)的⼆
# 维卷积层
conv2d = nn.Conv2D(1, kernel_size=(1, 2))
conv2d.initialize()
# ⼆维卷积层使⽤4维输⼊输出，格式为(样本, 通道, ⾼, 宽)，这⾥批量⼤⼩（批量中的样本数）和通
# 道数均为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
for i in range(10):
    with autograd.record():
        Y_hat = conv2d(X)
        l = (Y_hat - Y) ** 2
        l.backward()
# 简单起⻅，这⾥忽略了偏差
conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()
if (i + 1) % 2 == 0:
    print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))
```
4. 互相关运算和卷积运算

```
** 卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出
```
5. 特征图和感受野

```
** ⼆维卷积层输出的⼆维数组可以看作是输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图
** 影响元素x的前向计算的所有可能输⼊区域（可能⼤于输⼊的实际尺⼨）叫做x的感受野
```
## 填充和步幅
我们使⽤⾼和宽为3的输⼊与⾼和宽为2的卷积核得到⾼和宽为2的输出。⼀
般来说，假设输⼊形状是n h × n w ，卷积核窗口形状是k h × k w ，那么输出形状将会是：
```math
(n h − k h + 1) × (n w − k w + 1).
```
1. 填充

填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素），主要卷积核与原图像比例不对称时  
下⾯的例⼦⾥我们创建⼀个⾼和宽为3的⼆维卷积层，然后设输⼊⾼和宽两侧的填充数分别为1。
给定⼀个⾼和宽为8的输⼊，我们发现输出的⾼和宽也是8。
```
def comp_conv2d(conv2d, X):
    conv2d.initialize()
    # (1, 1)代表批量⼤⼩和通道数（“多输⼊通道和多输出通道”⼀节将介绍）均为1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:]) # 排除不关⼼的前两维：批量和通道
    # 注意这⾥是两侧分别填充1⾏或列，所以在两侧⼀共填充2⾏或列
conv2d = nn.Conv2D(1, kernel_size=3, padding=1)
X = nd.random.uniform(shape=(8, 8))
comp_conv2d(conv2d, X).shape
```

2. 步幅

```
** 卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。我们将每次滑动的⾏数和列数称为**步幅**
```
之前我们看到的都是步幅为1，但我们也可以一次性滑动多行多列  
⼀般来说，当⾼上步幅为s h ，宽上步幅为s w 时，输出形状为：
```math
⌊(n h − k h + p h + s h )/s h ⌋ × ⌊(n w − k w + p w + s w )/s w ⌋.

```

```
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)
print(comp_conv2d(conv2d, X).shape)
# 输出为(4,4)
conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))
comp_conv2d(conv2d, X).shape
#输出为（2,2）
```
## 多输⼊通道和多输出通道
前⾯两节⾥我们⽤到的输⼊和输出都是⼆维数组，但真实数据的维度经常更⾼。例如，彩⾊图像
在⾼和宽2个维度外还有RGB（红、绿、蓝**）3个颜⾊通道**。假设彩⾊图像的⾼和宽分别是h和w（像
素），那么它可以表⽰为⼀个3×h×w的多维数组。我们将⼤小为3的这⼀维称为通道（channel）
维
1. 多输⼊通道

当输⼊数据含多个通道时，我们需要构造⼀**个输⼊通道数与输⼊数据的通道数相同的卷积核**，从
而能够与含多通道的输⼊数据做互相关运算。由于输⼊和卷积核各有ci个通道，我们可以在各个通道上对**输⼊的⼆维数组和卷积核的⼆维核数组**做互相关运算

接下来我们实现含多个输⼊通道的互相关运算。我们只需要对每个通道做互相关运算，然后通
过add_n函数来进⾏累加：

```
def corr2d(X,K):
    h,w = K.shape
    Y = nd.zeros((X.shape[0]-h+1,X.shape[1]-w+1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] =(X[i: i + h, j: j + w] * K).sum()#本句话的意思
    return Y
def corr2d_multi_in(X, K):
    # ⾸先沿着X和K的第0维（通道维）遍历。然后使⽤*将结果列表变成add_n函数的位置参数
    # （positional argument）来进⾏相加
    return nd.add_n(*[corr2d(x, k) for x, k in zip(X, K)])#add_n函数的使用
X = nd.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = nd.array([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])
corr2d_multi_in(X, K)
```
2. 多输入通道
上面的不管我们的输入通道为多少，最后输出的通道数总是为1，如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为
```math
c i × k h × k w
```
 的核数组。将
它们在输出通道维上连结，卷积核的形状即
```math
c o × c i × k h × k w
```
。在做互相关运算时，每个输出通
道上的结果由卷积核在该输出通道上的核数组与整个输⼊数组计算而来。

```
这里的stack是什么？？？
# 2.多输出通道
def corr2d_multi_in_out(X, K):
    # 对K的第0维遍历，每次同输⼊X做互相关计算。所有结果使⽤stack函数合并在⼀起
    return nd.stack(*[corr2d_multi_in(X, k) for k in K])
K = nd.stack(K, K + 1, K + 2)#stack是什么意思？？
print(K.shape)
print(corr2d_multi_in_out(X, K))
```
3.  1 × 1卷积层

1 × 1卷积的主要计算发⽣在通道维上，主要目的是减少参数数量，降低计算复杂度。

下⾯我们使⽤全连接层中的矩阵乘法来实现1 × 1卷积。这⾥需要在矩阵乘法运算前后对数据形
状做⼀些调整。
```python
# 3.1X1卷积核
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))#3X9
    K = K.reshape((c_o, c_i))#2x3
    Y = nd.dot(K, X) # 全连接层的矩阵乘法
    return Y.reshape((c_o, h, w))
X = nd.random.uniform(shape=(3, 3, 3))
K = nd.random.uniform(shape=(2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)#2x3X3
Y2 = corr2d_multi_in_out(X, K)
(Y1 - Y2).norm().asscalar() < 1e-6
```
在之后的模型⾥我们将会看到1 × 1卷积层被当作保持⾼和宽维度形状不变的全连接层使⽤。于
是，我们可以通过调整⽹络层之间的通道数来控制模型复杂度
## 池化层
```
* 在本节中我们介绍池化（pooling）层，它的提出是为了缓解卷积层对位置的**过度敏感性**
```
1. ⼆维最⼤池化层和平均池化层

池化层直接计算池化窗口内元素的**最⼤值或者平均值**。该运算也分别叫做**最⼤池化或平均池化**。在⼆维最⼤池化中，池化窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输⼊⼦数组的最⼤值即输出数组中相应位置的元素。
![池化窗口形状为2 × 2的最⼤池化](http://qcihljxys.bkt.clouddn.com/1594104849%281%29.png)

如上图所示，我们使用了2X2最大池化，每一次取2X2范围内的最大值，之后再从左到右，从上到下移动

```math
max(0,1,3,4) = 4,
max(1,2,4,5) = 5,  
max(3,4,6,7) = 7,
max(4,5,7,8) = 8.

```

```python
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
print(pool2d(X, (2, 2)))
```
2. 填充和步幅

同卷积层⼀样，池化层也可以在输⼊的⾼和宽两侧的**填充并调整窗口的移动步幅**来改变输出形
状。池化层填充和步幅与卷积层填充和步幅的⼯作机制⼀样。我们将通过**nn模块⾥的⼆维最⼤池
化层MaxPool2D**来演⽰池化层填充和步幅的⼯作机制。我们先构造⼀个形状为(1, 1, 4, 4)的输⼊
数据，前两个维度分别是批量和通道。

```
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
print(pool2d(X, (2, 2)))

X = nd.arange(16).reshape((1, 1, 4, 4))
pool2d = nn.MaxPool2D(3)
pool2d(X) # 因为池化层没有模型参数，所以不需要调⽤参数初始化函数
pool2d = nn.MaxPool2D(3, padding=1, strides=2)#指定步幅和填充
print(pool2d(X))#填充是上下左右都会填充
```
也可以指定⾮正⽅形的池化窗口，并分别指定⾼和宽上的填充和步幅。
3. 多通道
## 卷积神经⽹络（LeNet）

LeNet 诞生于 1994 年,是最早的卷积神经网络之一，
每张图像⾼和宽均是28像素。我们将图像中的像素逐⾏展开，得到⻓度为784的向量，并输⼊进全连接层中,然而，这种分类⽅法有⼀定的局限:

```
* 图像在同⼀列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识
别
* 对于⼤尺⼨的输⼊图像，使⽤全连接层容易导致模型过⼤。假设输⼊是⾼和宽均为1,000像
素的彩⾊照⽚（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状也
是3,000,000 × 256：它占⽤了⼤约3 GB的内存或显存。这会带来过于复杂的模型和过⾼
的存储开销
```
卷积层尝试解决这两个问题。⼀⽅⾯，**卷积层保留输⼊形状**，使图像的像素在⾼和宽两个⽅向上
的相关性均可能被有效识别；另⼀⽅⾯，卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重
复计算，从而**避免参数尺⼨过⼤**


所以本节介绍一个卷积神经网络模型**Lenet**

```
* 1. 卷积层后接最⼤池化层,卷积层⽤来识别图像⾥的空间模式，如线条和物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性。
* 2. 每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数

```
```
#使用gpu计算（暂时没有GPU）
def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,
num_epochs):
    print('training on', ctx)
    loss = gloss.SoftmaxCrossEntropyLoss()
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()
        for X, y in train_iter:
            X, y = X.as_in_context(ctx), y.as_in_context(ctx)#将数据移到gpu上
            with autograd.record():
                y_hat = net(X)
                l = loss(y_hat, y).sum()
            l.backward()
        trainer.step(batch_size)
        y = y.astype('float32')
        #asscalar将向量X转换成标量，且向量X只能为一维含单个元素的向量
        train_l_sum += l.asscalar()
        train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
        n += y.size
        test_acc = evaluate_accuracy(test_iter, net, ctx)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '
            'time %.1f sec'
            % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,
            ))
```
## AlexNet
2012年，AlexNet横空出世。
主要有以下特征：

```
* 与相对较小的LeNet相⽐，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层
* 第⼀层中的卷积窗口形状是11×11，第⼆层中的卷积窗口形状减小到5×5，之后全采⽤3×3，卷积层之后都使⽤了窗口形状为3×3、步幅为2的最⼤池化层
* 最后⼀个卷积层的是两个输出个数为4096的全连接层
* AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数
* AlexNet通过丢弃法
* AlexNet引⼊了⼤量的图像增⼴，如翻转、裁剪和颜⾊变化，从而进⼀步扩⼤数据集来缓解过拟合
```
1. 构建模型
```
net = nn.Sequential()
# 使⽤较⼤的11 x 11窗⼝来捕获物体。同时使⽤步幅4来较⼤幅度减⼩输出⾼和宽。这⾥使⽤的输出通
# 道数⽐LeNet中的也要⼤很多
net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 减⼩卷积窗⼝，使⽤填充为2来使得输⼊与输出的⾼和宽⼀致，且增⼤输出通道数
        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 连续3个卷积层，且使⽤更⼩的卷积窗⼝。除了最后的卷积层外，进⼀步增⼤了输出通道数。
        # 前两个卷积层后不使⽤池化层来减⼩输⼊的⾼和宽
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 这⾥全连接层的输出个数⽐LeNet中的⼤数倍。使⽤丢弃层来缓解过拟合
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        # 输出层。由于这⾥使⽤Fashion-MNIST，所以⽤类别数为10，⽽⾮论⽂中的1000
        nn.Dense(10)
X = nd.random.uniform(shape=(1, 1, 224, 224))
net.initialize()
for layer in net:
    X = layer(X)
    print(layer.name, 'output shape:\t', X.shape)
```
2. 读取数据

在训练之前可以通过Resize来将Fashion-MNIST数据集扩⼤到AlexNet使⽤的图像⾼和宽224

```python
 # 本函数已保存在tools包中⽅便以后使⽤
def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(
        '~', '.mxnet', 'datasets', 'fashion-mnist')):
    root = os.path.expanduser(root) # 展开⽤⼾路径'~'
    transformer = []
    if resize:
        transformer += [gdata.vision.transforms.Resize(resize)]
    transformer += [gdata.vision.transforms.ToTensor()]
    transformer = gdata.vision.transforms.Compose(transformer)
    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)
    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)
    num_workers = 0 if sys.platform.startswith('win32') else 4
    train_iter = gdata.DataLoader(
        mnist_train.transform_first(transformer), batch_size, shuffle=True,
        num_workers=num_workers)
    test_iter = gdata.DataLoader(
        mnist_test.transform_first(transformer), batch_size, shuffle=False,
        num_workers=num_workers)
    return train_iter, test_iter

```
3. 训练模型


```python
lr, num_epochs, ctx = 0.01, 5,try_gpu()
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)
```
## 使⽤重复元素的⽹络（VGG）

```
2014年提出，使用很小的卷积（3*3），增加网络深度可以有效提升模型的效果，
网络结构更深，19层，使用3X3的卷积核 2个3X3可以代替5X5卷积核
多用1X1的卷积层可以了增加决策函数的非线性
优化方法(optimizer)是含有动量的随机梯度下降SGD+momentum(0.9)。
批尺寸(batch size)是256.
正则化(regularization):采用L2正则化，weight decay是5e-4。dropout在前两个全连接层后，p=0.5
```

```
def vgg_block(num_convs, num_channels):
    blk = nn.Sequential()
    for _ in range(num_convs):
        blk.add(nn.Conv2D(num_channels, kernel_size=3,
                            padding=1, activation='relu'))
    blk.add(nn.MaxPool2D(pool_size=2, strides=2))
    return blk
```
与AlexNet和LeNet⼀样，VGG⽹络由卷积层模块后接**全连接层**模块构成。卷积层模块**串联数
个vgg_block**，其超参数由变量conv_arch定义。该变量指定了每个VGG块⾥卷积层个数和
输出通道数。全连接模块则跟AlexNet中的⼀样。

现在我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第
⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积
层和3个全连接层，所以经常被称为VGG-11。

```
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
def vgg(conv_arch):
    net = nn.Sequential()
    # 卷积层部分
    for (num_convs, num_channels) in conv_arch:
        net.add(vgg_block(num_convs, num_channels))
    # 全连接层部分
    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
            nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
            nn.Dense(10))
    return net
net = vgg(conv_arch)
```
## ⽹络中的⽹络（NiN）

先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节我们介绍⽹络中的⽹络（NiN）[1]。它提出了另外⼀个思路，即**串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。**

```
NiN使⽤1×1卷积层来替代全连接层，从而使空间信息能够⾃然传递到后⾯的层中去
```
![image](http://qcihljxys.bkt.clouddn.com/1594197566%281%29.png)

上图是NiN结构同Alexnet,VGG结构的对比

```
** NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3×3的卷积层，相应的输出通道数也与AlexNet中的⼀致
** 每个NiN块后接⼀个步幅为2、窗口形状为3 × 3的最⼤池化层
```
1. 构建模型

```
net = nn.Sequential()
net.add(nin_block(96, kernel_size=11, strides=4, padding=0),
        nn.MaxPool2D(pool_size=3, strides=2),
        nin_block(256, kernel_size=5, strides=1, padding=2),
        nn.MaxPool2D(pool_size=3, strides=2),
        nin_block(384, kernel_size=3, strides=1, padding=1),
        nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),
        # 标签类别数是10
        nin_block(10, kernel_size=3, strides=1, padding=1),
        # 全局平均池化层将窗⼝形状⾃动设置成输⼊的⾼和宽
        nn.GlobalAvgPool2D(),
        # 将四维的输出转成⼆维的输出，其形状为(批量⼤⼩, 10)
        nn.Flatten())
```
## 含并⾏连结的⽹络(GoogLeNet)

GoogLeNet中的基础卷积块叫作**Inception块**,如下图为googlenet的网络结构图
![image](http://qcihljxys.bkt.clouddn.com/1594199789%281%29.png)

Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 ×
3和5 × 5的卷积层来抽取不同空间尺⼨下的信息，其中中间2个线路会对输⼊先做1 × 1卷积来减
少输⼊通道数，以降低模型复杂度。第四条线路则使⽤3×3最⼤池化层，后接1×1卷积层来改变通道数。4条线路都使⽤了合适的填充来使输⼊与输出的⾼和宽⼀致。最后我们将每条线路的输
出在通道维上连结，并输⼊接下来的层中去。

1.  Inception块

```
class Inception(nn.Block):
    # c1 - c4为每条线路⾥的层的输出通道数
    def __init__(self, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1 x 1卷积层
        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')
        # 线路2，1 x 1卷积层后接3 x 3卷积层
        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')
        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,
        activation='relu')
        # 线路3，1 x 1卷积层后接5 x 5卷积层
        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')
        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,
        activation='relu')
        # 线路4，3 x 3最⼤池化层后接1 x 1卷积层
        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)
        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')
    def forward(self, x):
        p1 = self.p1_1(x)
        p2 = self.p2_2(self.p2_1(x))
        p3 = self.p3_2(self.p3_1(x))
        p4 = self.p4_2(self.p4_1(x))
        return nd.concat(p1, p2, p3, p4, dim=1) # 在通道维上连结输出
```
2. GoogLeNet模型

GoogLeNet跟VGG⼀样，在主体卷积部分中使⽤**5个模块（block）**，每个模块之间使⽤**步幅为2的3×3最⼤池化层来减小输出⾼宽**。第⼀模块使⽤⼀个**64通道的7 × 7卷积层**。

```
#第一个模块
 b1 = nn.Sequential()
b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),
      nn.MaxPool2D(pool_size=3, strides=2, padding=1))
```
第⼆模块使⽤2个卷积层：⾸先是64通道的1×1卷积层，然后是将通道增⼤3倍的3×3卷积层。它
对应Inception块中的第⼆条线路
```
b2 = nn.Sequential()
b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),
    nn.Conv2D(192, kernel_size=3,padding=1,activation='relu'),
    nn.MaxPool2D(pool_size=3, strides=2, padding=1))
```
第三模块串联**2个完整的Inception块**。  
**第⼀个Inception块**的输出通道数为64+128+32+32 = 256，
其中4条线路的输出通道数⽐例为64 : 128 : 32 : 32 = 2 : 4 : 1 : 1。  
其中**第⼆、第三条线路**先分别将
输⼊通道数减小⾄96/192 = 1/2和16/192 = 1/12后，再接上第⼆层卷积层。第⼆个Inception块
输出通道数增⾄128 + 192 + 96 + 64 = 480，每条线路的输出通道数之⽐为128 : 192 : 96 : 64 =
4 : 6 : 3 : 2。其中第⼆、第三条线路先分别将输⼊通道数减小⾄128/256 = 1/2和32/256 = 1/8。

```
b3 = nn.Sequential()
b3.add(Inception(64, (96, 128), (16, 32), 32),
    Inception(128, (128, 192), (32, 96), 64),
    nn.MaxPool2D(pool_size=3, strides=2, padding=1))
```
第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + 208 + 48 +64=512、160+224+64+64=512、128+256+64+64 = 512、112+288+64+64 = 528和256+320+128+128 =
832。这些线路的通道数分配和第三模块中的类似，⾸先含3 × 3卷积层的第⼆条线路输出最多通
道，其次是仅含1×1卷积层的第⼀条线路，之后是含5×5卷积层的第三条线路和含3×3最⼤池化
层的第四条线路。其中第⼆、第三条线路都会先按⽐例减小通道数。这些⽐例在各个Inception块中都略有不同。

```
b4 = nn.Sequential()
b4.add(Inception(192, (96, 208), (16, 48), 64),
        Inception(160, (112, 224), (24, 64), 64),
        Inception(128, (128, 256), (24, 64), 64),
        Inception(112, (144, 288), (32, 64), 64),
        Inception(256, (160, 320), (32, 128), 128),
        nn.MaxPool2D(pool_size=3, strides=2, padding=1))
```
第五模块有输出通道数为256 + 320 + 128 + 128 = 832和384 + 384 + 128 + 128 = 1024的两
个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的⼀致，只是在具体数值
上有所不同。需要注意的是，第五模块的后⾯紧跟输出层，该模块同NiN⼀样使⽤全局平均池化
层来将每个通道的⾼和宽变成1。最后我们将输出变成⼆维数组后接上⼀个输出个数为标签类别
数的全连接层。

```
b5 = nn.Sequential()
b5.add(Inception(256, (160, 320), (32, 128), 128),
        Inception(384, (192, 384), (48, 128), 128),
        nn.GlobalAvgPool2D())
net = nn.Sequential()
net.add(b1, b2, b3, b4, b5, nn.Dense(10))
```
演示每个块模型

```
X = nd.random.uniform(shape=(1, 1, 96, 96))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
```
3. 训练数据和训练模型

```
lr, num_epochs, batch_size, ctx = 0.1, 5, 128, d2l.try_gpu()
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,
        num_epochs)
```
## 批量归⼀化

**目的**：让神经网络更加容易训练；  
**大致过程**：在模型训练时，批量归⼀化利⽤小批量上
的均值和标准差，不断调整神经⽹络中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定
**具体做法**：
- 求均值

![image](http://qcihljxys.bkt.clouddn.com/1594277948%281%29.png)
- 求方差

![image](http://qcihljxys.bkt.clouddn.com/1594277963%281%29.png)
- 标准化

![image](http://qcihljxys.bkt.clouddn.com/1594277975%281%29.png)
1. 代码实现

```
class BatchNorm(nn.Block):
    def __init__(self, num_features, num_dims, **kwargs):
        super(BatchNorm, self).__init__(**kwargs)
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1
        self.gamma = self.params.get('gamma', shape=shape, init=init.One())
        self.beta = self.params.get('beta', shape=shape, init=init.Zero())
        # 不参与求梯度和迭代的变量，全在内存上初始化成0
        self.moving_mean = nd.zeros(shape)
        self.moving_var = nd.zeros(shape)
    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上
        if self.moving_mean.context != X.context:
            self.moving_mean = self.moving_mean.copyto(X.context)
            self.moving_var = self.moving_var.copyto(X.context)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma.data(), self.beta.data(), self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```
2. 使⽤批量归⼀化层的LeNet  

**注意**：对全连接层和卷积层做批量归一化不同
```
net = nn.Sequential()
net.add(nn.Conv2D(6, kernel_size=5),
        nn.BatchNorm(),
        nn.Activation('sigmoid'),
        nn.MaxPool2D(pool_size=2, strides=2),
        nn.Conv2D(16, kernel_size=5),
        nn.BatchNorm(),
        nn.Activation('sigmoid'),
        nn.MaxPool2D(pool_size=2, strides=2),
        nn.Dense(120),
        nn.BatchNorm(),
        nn.Activation('sigmoid'),
        nn.Dense(84),
        nn.BatchNorm(),
        nn.Activation('sigmoid'),
        nn.Dense(10))
```
##  残差⽹络（ResNet）
![image](http://qcihljxys.bkt.clouddn.com/1594368913%281%29.png)
1. 定义resnet残差块

```
class Residual(nn.Block): # 本类已保存在d2lzh包中⽅便以后使⽤
    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
        strides=strides)
        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
            strides=strides)
        else:
            self.conv3 = None
            self.bn1 = nn.BatchNorm()
            self.bn2 = nn.BatchNorm()
    def forward(self, X):
        Y = nd.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return nd.relu(Y + X)
```
2. ResNet模型结构
如上图所示，不管resnet网络有多少层，前两层都是：1.输出通道数为64、步幅为2的7 × 7卷积层 2.步幅为2的3 ×3的最⼤池化层。
```
net = nn.Sequential()
net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
        nn.BatchNorm(), nn.Activation('relu'),
        nn.MaxPool2D(pool_size=3, strides=2, padding=1))
```
再接着，ResNet则使⽤4个由残差块组成的模块，每
个模块使⽤若⼲个同样输出通道数的残差块。第⼀个模块的通道数同输⼊通道数⼀致。由于之前
已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。之后的每个模块在第⼀个残差块⾥将
上⼀个模块的通道数翻倍，并将⾼和宽减半。

```
def resnet_block(num_channels, num_residuals, first_block=False):
    blk = nn.Sequential()
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.add(Residual(num_channels))
    return blk

net.add(resnet_block(64, 2, first_block=True),
        resnet_block(128, 2),
        resnet_block(256, 2),
        resnet_block(512, 2))
```
最后，加⼊全局平均池化层后接上全连接层输出。

```
net.add(nn.GlobalAvgPool2D(), nn.Dense(10))
```

## 稠密连接⽹络（DenseNet）
