# 优化算法
在训练模型时，我们会使⽤优化算法不断迭代模型参数以**降低模型损失函数的值**。当迭代终⽌时，模型的训练随之终⽌，此时的模型参数就是模型通过训练所学习到的参数

本章将详细介绍深度学习中常⽤的**优化算法**
## 优化与深度学习
在⼀个深度学习问题中，我们通常会预先定义⼀个损失函数。有了损失函数以后，我们就可以==使⽤优化算法试图将其最小化==。
在优化中，这样的损失函数通常被称作优化问题的==⽬标函数==（objective function）。依据惯例，++优化算法通常只考虑最小化⽬标函数++
### 1. 局部最小值和鞍点
- 一个函数可能存在多个局部最小值点，对目标函数的最小化很可能使函数陷入局部最小值，而不能到达全局最小值
- 刚刚我们提到，梯度接近或变成零可能是由于当前解在局部最优解附近造成的。事实上，另⼀种可能性是当前解在**鞍点（saddlepoint）附近**，如下图所示：
![image](http://qcihljxys.bkt.clouddn.com/1594967548%281%29.png)
### 2. 梯度下降和随机梯度下降
==梯度下降==：设置一个**学习率**，然后每次更新当前函数值，直到本次更新值与原值差别很小，就认为已经到达了最小值点

```python
def gd(eta):
    x = 10
    results = [x]
    for i in range(10):
        x -= eta * 2 * x # f(x) = x * x的导数为f'(x) = 2 * x
        results.append(x)
    print('epoch 10, x:', x)
    return results
res = gd(0.2)
```
== 随机梯度下降==：如果使⽤梯度下降，每次⾃变量迭代的计算开销为O(n)，它随着n线性增⻓。因此，当训练数据样本数很⼤时**，梯度下降每次迭代的计算开销很⾼**。随机梯度下降（SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们**随机均匀采样的⼀个样本索引i**∈{1,...,n}，根据随机选取样本数据来计算迭代梯度
### ⼩批量随机梯度下降
在每⼀次迭代中，**梯度下降**使⽤整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降
（batch gradient descent）。而**随机梯度下降**在每次迭代中只随机采样⼀个样本来计算梯度。正如
我们在前⼏章中所看到的，我们还可以在每轮迭代中随机均匀采样多个样本来组成⼀个小批量，然后使⽤这个小批量来计算梯度，也就是**小批量梯度下降算法**
## 优化算法调用
### 动量法
动量法的提出是为了解决不同方向的自变量梯度大小不同，而导致的在不同方向的移动幅度不同

动量法在每个时间步的⾃变量更新量近似于将前者对应的最近1/(1 − γ)个时间步的更新量做了指数加权移动平均后再除以1−γ。所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致。
###  AdaGrad算法
在“动量法”⼀节⾥我们看到当x 1 和x 2 的梯度值有较⼤差别时，需要选择⾜够小的学习率使得⾃变量在梯度值较⼤的维度上不发散。但这样会导**致⾃变量在梯度值较小的维度上迭代过慢**。动量法依赖**指数加权移动平均**使得⾃变量的更新⽅向更加⼀致，从而降低发散的可能。本节我们介绍AdaGrad算法，它根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题 [1]。
1. 算法

```math
x_t ← x_{t−1} −{η\over{√ st + ϵ}}⊙ g t ,
```
通过累加来缩小η

但是，小批量随机梯度按元素平⽅的累加变量s_t出现在学习率的分⺟项中。因此，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢。然
而，由于s_t ⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于**学习率过小**，可能较难找到⼀个有⽤的解
###  RMSProp算法
上面的AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。为了解决这⼀问题，++RMSProp算法对AdaGrad算法做了⼀点小小的修改++
虽然RMSProp算法的公式与AdaGrad算法相同，但RMSProp算法的状态变量s_t 是对平⽅项g t ⊙ g t 的***指数加权移动平均***,

---

==所以 RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使⽤了小批量随机梯度按元素平⽅的指数加权移动平均来调整学习率。==
### AdaDelta算法
Adadelta的分母和RMSProp的分母一致
需要更新参数的变化量为:
![image](http://qcihljxys.bkt.clouddn.com/20190801211526954.png)

分子表示的是每次更新梯度变化量的累加量:
![image](http://qcihljxys.bkt.clouddn.com/201908012117035.png)

###  ==Adam算法==（没懂）
时间步t的动量变量v t 即小批量随机梯度g t 的指数加权移动平均：

```math
v_t ← β_1 v_{t−1} + (1 − β_1 )g_t .
```
