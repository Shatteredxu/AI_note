# 深度学习基础
## 线性回归从零开始实现
### 线性回归基本要素
1. 模型：输入输出是线性关系
2. 训练：找到一组最好的参数值
3. 数据：输入和输出的对应值
4. 损失函数：通过最小化损失函数来找到一组最好的参数值，常用MSE均方误差
5. 优化算法：寻找最优参数值的算法，常用梯度下降算法—>小批量梯度下降（先选取⼀组模型参数的初始值，如随机选取；接下来对
参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减小量。）
6. 模型预测：得到优化算法所计算出来的参数，用训练出来的模型来预测
### 线性回归的表示方法
1.神经网络图：

```
* 线性回归是⼀个单层神经⽹络。输出层中负责计算o的单元⼜叫神经元。
* 在线性回归中，o的计算依赖于x1 和x2。也就是说，输出层中的神经元和输⼊层中各个输⼊完全连接。因此，这⾥的输出层⼜叫全连接层（fully-connected layer）或稠密层（dense layer）
```
### 矢量计算表达式
矢量计算机方法主要有两种

```
* 第一种：对应数组中的值逐个相加，
* 第二种：直接相加
```
后者性能优于前者
### 线性回归从头实现
1.生成数据集
![image](http://qcihljxys.bkt.clouddn.com/1593157901%281%29.png)
2.用matplot查看一下数据集的分布（pyplot应用）
```
def use_svg_display():
    display.set_matplotlib_formats('svg')
def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺⼨
    plt.rcParams['figure.figsize'] = figsize
    set_figsize()
    plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1);
```
3.读取数据

```python
def data_iter(batch_size,features,labels):
    num_examples=len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        j = nd.array(indices[i: min(i + batch_size, num_examples)])
        yield features.take(j), labels.take(j) # take函数根据索引返回
```
新建tools文件，data_iter方进去，通过from tools import data_iter导入, 调用 

```python
batch_size=10;# 需要查看多少个数据
for X, y in data_iter(batch_size, features, labels):
    print(X, y)
    break
```
4.初始化模型参数
```python
# 初始化w,b参数
 w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))#num_inputs为2，分别创建2维的w矩阵
 b = nd.zeros(shape=(1,))
 #分别创建他们的梯度
 w.attach_grad()
b.attach_grad()
```
5.定义模型
```
def linreg(X, w, b): # 本函数已保存在tools文件中直接调用
    return nd.dot(X, w) + b
```
6.定义损失函数

```math
MSE=(y-y*)^2/2
```

```python
def squared_loss(y_hat, y): # 本函数已保存在tools文件中直接调用
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
    # y_hat表示预测值，y表示真实值
```
7.定义优化算法

```python
def sgd(params, lr, batch_size):  # 本函数已保存在tools文件中直接调用
    for param in params:
        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.dat
```
lr代表学习率， param.grad代表自动求导，利用循环来更新每一个参数值  

8.训练模型
- 读取的小批量数据样本（特
征X和标签y）
- 调⽤反向函数backward计算小批量随机梯度，并调⽤优化算法sgd迭代模型参数

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
for epoch in range(num_epochs): # 训练模型⼀共需要num_epochs个迭代周期
# 在每⼀个迭代周期中，会使⽤训练数据集中所有样本⼀次（假设样本数能够被批量⼤⼩整除）。X
# 和y分别是⼩批量样本的特征和标签
for X, y in data_iter(batch_size, features, labels):
    with autograd.record():
        l = loss(net(X, w, b), y) #l是有关⼩批量X和y的损失
    l.backward() # ⼩批量的损失对模型参数求梯度
    sgd([w, b], lr, batch_size) # 使⽤⼩批量随机梯度下降迭代模型参数
train_l = loss(net(features, w, b), labels)
print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))
```
#### 全部代码
```
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd,nd
import random
from tools import data_iter,squared_loss,linreg,sgd
#生成数据集
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
#生成x
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
# 生成y=w1x1+w2x2+b
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
# 添加噪声
labels += nd.random.normal(scale=0.01, shape=labels.shape)
def use_svg_display():
    display.set_matplotlib_formats('svg')
def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺⼨
    plt.rcParams['figure.figsize'] = figsize

set_figsize()
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1); # 加分号只显⽰图

w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))#num_inputs为2，分别创建2维的w矩阵
b = nd.zeros(shape=(1,))
#分别创建他们的梯度
w.attach_grad()
b.attach_grad()
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
batch_size = 10

for epoch in range(num_epochs):

    for X, y in data_iter(batch_size, features, labels):
        with autograd.record():
            l = loss(net(X, w, b), y) #l是有关⼩批量X和y的损失
        l.backward() # ⼩批量的损失对模型参数求梯度
        sgd([w, b], lr, batch_size) # 使⽤⼩批量随机梯度下降迭代模型参数
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))
print(true_w,w,true_b,b)
```
#### 练习(todo)
- 为什么squared_loss函数中需要使⽤reshape函数？
- 尝试使⽤不同的学习率，观察损失函数值的下降快慢。
-  如果样本个数不能被批量⼤小整除，data_iter函数的⾏为会有什么变化？
## 线性回归的简单实现
1. 生成数据集(同上一节)
```
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)
```

2. 读取数据  

```
PyTorch提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用Data代替。
```
在每一次迭代中，我们将随机读取包含10个数据样本的小批量。
```python
import torch.utils.data as Data
batch_size = 10
# 将训练数据的特征和标签组合
dataset = Data.TensorDataset(features, labels)
# 随机读取小批量
data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)
```
测试一下

```
for X, y in data_iter:
    print(X, y)
    break
```
3. 定义模型
这节代码将会更加简洁，首先导入
```torch.nn```模块该模块定义了大量神经网络的层。

```
# 3.模型定义
# 继承nn.Module，重写自己的网络
#可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。
import torch.nn as nn
net = nn.Sequential(
    nn.Linear(num_inputs, 1)
    # 此处还可以传入其他层
    )
#测试编写的网络结构，输入训练数据
print(net[0]) # 使用print可以打印出网络的结构
```
4. 初始化模型参数

```python
from torch.nn import init

init.normal_(net[0].weight, mean=0, std=0.01)
#init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。
init.constant_(net[0].bias, val=0)  
# 用值val填充向量bias
```
5. 定义损失函数

```
loss = nn.MSELoss()
```

6. 定义优化算法
 ```
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.03)
#学习率0.03，net.parameters()初始化参数，采用SGD优化算法
print(optimizer)
```
如果动态学习率怎么弄？

```
# 调整学习率
for param_group in optimizer.param_groups:
    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍
```

7. 训练模型

```
# 7.开始训练
num_epochs = 3
for epoch in range(1,num_epochs+1):
    for X,y in data_iter:
        output = net(X)
        l = loss(output,y.view(-1,1))
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
    print('epoch %d, loss: %f' % (epoch, l.item()))
# 8.打印参数
dense = net[0]
print(true_w, dense.weight)
print(true_b, dense.bias)
```
8. 全部代码总结

```python
import torch
from IPython import display
from matplotlib import pyplot as plt
import numpy as np
import random


# 1.生成数据集
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)


#2.读取数据
import torch.utils.data as Data
batch_size = 10
# 将训练数据的特征和标签组合
dataset = Data.TensorDataset(features, labels)
# 随机读取小批量
data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)


# 3.模型定义
# 继承nn.Module，重写自己的网络
#可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。
import torch.nn as nn
net = nn.Sequential(
    nn.Linear(num_inputs, 1)
    # 此处还可以传入其他层
    )
#测试编写的网络结构，输入训练数据
print(net[0]) # 使用print可以打印出网络的结构


# 4.使用init库初始化模型参数
from torch.nn import init
init.normal_(net[0].weight,mean=0,std=0.01)
init.constant_(net[0].bias,val=0)


# 5.定义损失函数MSE
loss = nn.MSELoss()


# 6.定义优化算法
import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.03)
print(optimizer)


# 7.开始训练
num_epochs = 3
for epoch in range(1,num_epochs+1):
    for X,y in data_iter:
        output = net(X)
        l = loss(output,y.view(-1,1))#view效果通reshape，就是改变y的形状
        optimizer.zero_grad()#梯度清零
        l.backward()#反向传播
        optimizer.step()#迭代更新参数
    print('epoch %d, loss: %f' % (epoch, l.item()))
    

# 8.打印参数
dense = net[0]
print(true_w, dense.weight)
print(true_b, dense.bias)
```
## softmax回归
引入softmax主要用于输出为离散值的分类问题，前几节的回归模型适用于输出为连续值的情景
### softmax模型
就是将输出的离散值，归入一个区间中，他们所有的相加得1，也就是概率
- [ ] ###  交叉熵损失函数(有神马用处)
```math
y_i= {exp(o_1)\over \sum_{i=0}^n exp(o_i)}
```
- softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。
- softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。
- 交叉熵适合衡量两个概率分布的差异。

## 图像分类数据集（Fashion-MNIST）
后面的数据集用一个图像内容更加复杂的数据集Fashion-MNIST[2]（这个数据集也比较小，只有几十M，没有GPU的电脑也能吃得消）  

本节我们将使用```torchvision```
包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：
1. ```torchvision.datasets```:一些加载数据的函数及常用的数据集接口；
2. ```torchvision.models```: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；
3. ```torchvision.transforms```: 常用的图片变换，例如裁剪、旋转等；
4.  ```torchvision.utils```: 其他的一些有用的方法
### 获取数据集
1. 导入所需要的包
```
import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import time
import sys
sys.path.append("..") # 为了导入上层目录的d2lzh_pytorch
import d2lzh_pytorch as d2l
```
2.下载数据集
第一次会花费较长时间去下载
```python
mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())
mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())
print(type(mnist_train))#<class 'torchvision.datasets.mnist.FashionMNIST'>
print(len(mnist_train), len(mnist_test))#打印数据集训练样本有60000，测试样本有10000
feature, label = mnist_train[0]#获取第一个样本数据
print(feature.shape, label)#打印的是1X28X28的CxHxW

```
![image](http://qcihljxys.bkt.clouddn.com/1593224327%281%29.png)
下载完成后面可以通过```show_fashion_mnist```方法调用一次显示10张图片，代码如下

```python
X, y = [], []
for i in range(10):
    X.append(mnist_train[i][0])#X代表训练数据集的feature标签
    y.append(mnist_train[i][1])#y代表训练数据集的label标签
show_fashion_mnist(X, get_fashion_mnist_labels(y))
```python
3.利用pytorh读取部分数据
在使用时，利用```torch.utils.data.DataLoader```中的方法来小批量读取一部分数据，在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的**DataLoader**中一个很方便的功能是允许使用**多进程**来加速数据读取。这里我们通过参数num_workers来设置4个进程读取数据
```# 利用dataloader读取小批量数据
batch_size = 256#一次放入256个数据
if sys.platform.startswith("win"):
    num_workers = 0
else:
    num_workers = 4
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    # shuffle=true代表随机选取
start = time.time()
for X, y in train_iter:
    continue
print('%.2f sec' % (time.time() - start))#结果：8.83 sec
```
## softmax回归的从零开始实现
1. 获取和导入数据
```
import torch
import torchvision
import numpy as np
import sys
from getDatasets import load_data_fashion_mnist
# 1. 获取数据
batch_size = 256
train_iter,test_iter =load_data_fashion_mnist(batch_size)
```
2. 初始化模型参数
> 每个样本输入是高和宽均为28像素的图像。模型的输入向量的长度是```28×28=784```：该向量的每个元素对应图像中每个像素。由于图像有10个类别，单层神经网络输出层的输出个数为10，因此softmax回归的权重和偏差参数分别为784×10和1×10的矩阵
- [ ] requires_grad_的作用
```python
num_inputs = 784
num_outputs = 10
#随机生成初始化参数w,b
W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)
b = torch.zeros(num_outputs, dtype=torch.float)

W.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True)
# requires_grad_()的主要用途是告诉自动求导开始记录对Tensor的操作。
```
3. 实现softmax运算
sum函数的使用：传入dim=i维度，i=0，代表列维度相加；i=1代表行维度相加

```python
def softmax(x):
    X_exp = x.exp()
    partition = X_exp.sum(dim=1,keepdim=True)
    return X_exp/partition
X = torch.rand((2,5)) #生成2X5的矩阵，从区间[0, 1)的均匀分布中抽取的一组随机数
# randn 返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。
X_prob=softmax(X)
print(X_prob,"----",X_prob.sum(dim=1))

```
4. 定义模型
- ```torch.mul(a, b)```是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵
- ```torch.mm(a, b)```是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵

所以mm方法的作用就是将w与x相乘+b，侯建y=wx+b的模型
view方法同numpy中的reshape，改变矩阵的形状

```python
def net(X):
    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)
```
5. 损失函数的定义
- [ ] gather函数的使用

```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat.gather(1, y.view(-1, 1)))
```
6. 计算分类准确率

```python
def accuracy(y_hat, y):
    return (y_hat.argmax(dim=1) == y).float().mean().item()
```
7.训练模型

```
num_epochs, lr = 5, 0.1

# 本函数已保存在d2lzh包中方便以后使用
def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
              params=None, lr=None, optimizer=None):
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y).sum()

            # 梯度清零
            if optimizer is not None:
                optimizer.zero_grad()
            elif params is not None and params[0].grad is not None:
                for param in params:
                    param.grad.data.zero_()

            l.backward()
            if optimizer is None:
                d2l.sgd(params, lr, batch_size)
            else:
                optimizer.step()  # “softmax回归的简洁实现”一节将用到


            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))

train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)

```

8.预测结果

```
#8.训练结束后进行结果预测
X, y = iter(test_iter).next()

true_labels = get_fashion_mnist_labels(y.numpy())
pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())
titles = [true + '\n' + pred for true, pred in zip(true_labels, pred_labels)]

show_fashion_mnist(X[0:9], titles[0:9])
```


